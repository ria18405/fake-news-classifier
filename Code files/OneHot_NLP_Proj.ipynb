{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OneHot_NLP_Proj.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5LF7xmlrn3S",
        "outputId": "33952056-beee-4e85-afe7-bde6d973498e"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from textblob import TextBlob, Word, Blobber\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "# print(stopwords.words('english'))\n",
        "import plotly.express as px\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "nltk.download('stopwords')\n",
        "from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, RepeatedStratifiedKFold, RandomizedSearchCV, LeavePOut, StratifiedKFold\n",
        "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, RandomForestClassifier, BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.metrics as metrics"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WR1hr0PKpDlY",
        "outputId": "2fe013f7-bae0-4747-cfaf-2251b453d363"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqxJmtHPDd-i"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/NLP/NLP project/train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzFWvL-CEu-R"
      },
      "source": [
        "df = df.dropna()\n",
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8J3sADXV3CRr",
        "outputId": "47923b7b-93d3-497c-b673-bbab3162bd80"
      },
      "source": [
        "from tqdm.notebook import tqdm, trange\n",
        "from nltk.stem import PorterStemmer\n",
        "!pip install autocorrect\n",
        "from autocorrect import Speller\n",
        "from autocorrect import Speller\n",
        "\n",
        "spell = Speller(lang='en')\n",
        "\n",
        "porter = PorterStemmer()\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "def lower_text(text):\n",
        "    return text.lower()\n",
        "\n",
        "def stemSentence(sentence):\n",
        "    token_words=word_tokenize(sentence)\n",
        "    token_words\n",
        "    stem_sentence=[]\n",
        "    for word in token_words:\n",
        "        stem_sentence.append(porter.stem(word))\n",
        "        stem_sentence.append(\" \")\n",
        "    return \"\".join(stem_sentence)\n",
        "\n",
        "def lemmatization(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "\n",
        "def remove_numbers(text):\n",
        "    text_cleaned = re.sub('[^a-zA-Z]',' ',text)\n",
        "    return text_cleaned\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stoplist = stopwords.words('english')\n",
        "    sps = stoplist\n",
        "    return \" \".join([word for word in str(text).split() if word not in sps])\n",
        "\n",
        "def autospell(text):\n",
        "    return \" \".join([spell(word) for word in text.split()])\n",
        "\n",
        "corpus=[]\n",
        "def get_corpus(sentence):\n",
        "    cps = []\n",
        "    token_words=word_tokenize(sentence)\n",
        "    for word in token_words:\n",
        "        corpus.append(word)\n",
        "    return sentence\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autocorrect\n",
            "  Downloading autocorrect-2.6.0.tar.gz (622 kB)\n",
            "\u001b[?25l\r\u001b[K     |▌                               | 10 kB 33.5 MB/s eta 0:00:01\r\u001b[K     |█                               | 20 kB 35.1 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 30 kB 38.5 MB/s eta 0:00:01\r\u001b[K     |██                              | 40 kB 29.1 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 51 kB 17.6 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 61 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 71 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 81 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 92 kB 13.2 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 102 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 112 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 122 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 133 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 143 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |████████                        | 153 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 163 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 174 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 184 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 194 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 204 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 215 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 225 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 235 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 245 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 256 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 266 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 276 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 286 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 296 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 307 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 317 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 327 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 337 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 348 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 358 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 368 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 378 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 389 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 399 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 409 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 419 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 430 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 440 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 450 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 460 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 471 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 481 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 491 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 501 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 512 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 522 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 532 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 542 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 552 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 563 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 573 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 583 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 593 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 604 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 614 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 622 kB 11.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.6.0-py3-none-any.whl size=622249 sha256=4853dcefd799e837aba3f5ee53c463bf739bf44dcea21c6984f26ec97d67652c\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/ce/aa/bc894efbe0541ce91dea21561d01d319783986d9787a8e9f58\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwJt25e23EBj"
      },
      "source": [
        "df['text'] = df['text'].apply(remove_stopwords)\n",
        "# df['title'] = df['title'].apply(remove_stopwords)\n",
        "# df['author'] = df['author'].apply(remove_stopwords)\n",
        "\n",
        "df['text'] = df['text'].apply(remove_numbers)\n",
        "# df['title'] = df['title'].apply(remove_numbers)\n",
        "# df['author'] = df['author'].apply(remove_numbers)\n",
        "\n",
        "df['text'] = df['text'].apply(lemmatization)\n",
        "# df['title'] = df['title'].apply(lemmatization)\n",
        "# df['author'] = df['author'].apply(lemmatization)\n",
        "\n",
        "df['text'] = df['text'].apply(lower_text)\n",
        "# df['title'] = df['title'].apply(lower_text)\n",
        "# df['author'] = df['author'].apply(lower_text)\n",
        "\n",
        "# df['text'] = df['text'].apply(get_corpus)\n",
        "# df['title'] = df['title'].apply(get_corpus)\n",
        "# df['author'] = df['author'].apply(get_corpus)\n",
        "# corpus = set(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBrBO8N3nXMa",
        "outputId": "4b591a57-d06a-4717-ec63-ff56422529cb"
      },
      "source": [
        "# dropping length 0, 1, 2 texts\n",
        "ohr = []\n",
        "for index in range(len(df)):\n",
        "    # ohr.append(len(onehot_rep_test[index]))\n",
        "    if len(df['text'][index])==0 or len(df['text'][index])==1 or len(df['text'][index])==2:\n",
        "        ohr.append(index)\n",
        "print(len(ohr), len(df))\n",
        "df = df.drop(ohr)        \n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "print(len(df))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "93 18285\n",
            "18192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxim30_dm5oq"
      },
      "source": [
        "-------------------------------Preprocessing ends----------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxG7R9HOzlc6"
      },
      "source": [
        "X = df['text']\n",
        "y = df['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
        "X_train = X_train.sample(frac=1).reset_index(drop=True)\n",
        "X_test = X_test.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3jkV7B71Bsk"
      },
      "source": [
        "voc_size = 5000\n",
        "onehot_rep = [one_hot(sentence,voc_size)for sentence in X_train]\n",
        "onehot_rep_test = [one_hot(sentence,voc_size)for sentence in X_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLksMCbi8Fnb",
        "outputId": "beef895c-597c-41c1-96ee-c5836e1c9f99"
      },
      "source": [
        "len(onehot_rep), len(onehot_rep[0]), len(onehot_rep[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14553, 188, 1020)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFx3d5Q_8Ttq"
      },
      "source": [
        "maxlen = 386"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsLEaeV53xsw"
      },
      "source": [
        "one_hot_embeddings_train = pad_sequences(onehot_rep,padding='pre',maxlen=maxlen)\n",
        "one_hot_embeddings_test = pad_sequences(onehot_rep_test,padding='pre',maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glHYuTCc8XAs",
        "outputId": "c30e0564-0ac6-4378-b825-a72174306748"
      },
      "source": [
        "len(one_hot_embeddings_train), len(one_hot_embeddings_train[0]), len(one_hot_embeddings_train[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14553, 386, 386)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-S4dvFzXe_V"
      },
      "source": [
        "-----------------vectorization (OneHot) ends -----------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTho9gkC8gjS"
      },
      "source": [
        "X_train = one_hot_embeddings_train\n",
        "X_test = one_hot_embeddings_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdkpIteWrV4y",
        "outputId": "b8472988-d46f-47ae-c799-d68fe6a27d68"
      },
      "source": [
        "naive_bayes_classifier = MultinomialNB()\n",
        "naive_bayes_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = naive_bayes_classifier.predict(X_test)\n",
        "\n",
        "score1 = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"accuracy:   %0.3f\" % score1)\n",
        "\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"confusion matrix:\")\n",
        "print(metrics.confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print('------------------------------')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy:   0.513\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.54      0.56      2076\n",
            "           1       0.44      0.47      0.46      1581\n",
            "\n",
            "    accuracy                           0.51      3657\n",
            "   macro avg       0.51      0.51      0.51      3657\n",
            "weighted avg       0.52      0.51      0.51      3657\n",
            "\n",
            "confusion matrix:\n",
            "[[1131  945]\n",
            " [ 836  745]]\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujVM92NqrX1a",
        "outputId": "af32d5ec-2e87-4b8b-ab97-7a98e30f9562"
      },
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "clf = make_pipeline(SVC(gamma='auto', probability=True))\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# compute the performance measures\n",
        "score1 = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"accuracy:   %0.3f\" % score1)\n",
        "\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"confusion matrix:\")\n",
        "print(metrics.confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print('------------------------------')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy:   0.561\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.99      0.72      2052\n",
            "           1       0.37      0.01      0.02      1587\n",
            "\n",
            "    accuracy                           0.56      3639\n",
            "   macro avg       0.47      0.50      0.37      3639\n",
            "weighted avg       0.48      0.56      0.41      3639\n",
            "\n",
            "confusion matrix:\n",
            "[[2028   24]\n",
            " [1573   14]]\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3moKcFpkGZS",
        "outputId": "ca24afd2-74ae-4ad2-c1ff-801c46b2be70"
      },
      "source": [
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "# compute the performance measures\n",
        "score1 = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"accuracy:   %0.3f\" % score1)\n",
        "\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"confusion matrix:\")\n",
        "print(metrics.confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy:   0.513\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.57      0.57      2052\n",
            "           1       0.44      0.44      0.44      1587\n",
            "\n",
            "    accuracy                           0.51      3639\n",
            "   macro avg       0.50      0.50      0.50      3639\n",
            "weighted avg       0.51      0.51      0.51      3639\n",
            "\n",
            "confusion matrix:\n",
            "[[1168  884]\n",
            " [ 888  699]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiJ21hNYlHIk",
        "outputId": "0a9981e2-e419-4e24-f80f-ff745cab2b66"
      },
      "source": [
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# compute the performance measures\n",
        "score1 = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"accuracy:   %0.3f\" % score1)\n",
        "\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"confusion matrix:\")\n",
        "print(metrics.confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy:   0.545\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.80      0.67      2104\n",
            "           1       0.42      0.20      0.27      1535\n",
            "\n",
            "    accuracy                           0.54      3639\n",
            "   macro avg       0.50      0.50      0.47      3639\n",
            "weighted avg       0.51      0.54      0.50      3639\n",
            "\n",
            "confusion matrix:\n",
            "[[1677  427]\n",
            " [1230  305]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4WUkRTJlaJK",
        "outputId": "1a186cf7-744a-43da-b47c-4fe0cbc712d8"
      },
      "source": [
        "clf = AdaBoostClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "# compute the performance measures\n",
        "score1 = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"accuracy:   %0.3f\" % score1)\n",
        "\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"confusion matrix:\")\n",
        "print(metrics.confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy:   0.555\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.82      0.68      2104\n",
            "           1       0.44      0.19      0.26      1535\n",
            "\n",
            "    accuracy                           0.55      3639\n",
            "   macro avg       0.51      0.51      0.47      3639\n",
            "weighted avg       0.52      0.55      0.51      3639\n",
            "\n",
            "confusion matrix:\n",
            "[[1727  377]\n",
            " [1243  292]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UczphPWnmD3c",
        "outputId": "0b4b88cc-3cea-4c3a-dec2-b3ca2d37f9b2"
      },
      "source": [
        "clf = MLPClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "# compute the performance measures\n",
        "score1 = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"accuracy:   %0.3f\" % score1)\n",
        "\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"confusion matrix:\")\n",
        "print(metrics.confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy:   0.539\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.66      0.62      2104\n",
            "           1       0.45      0.38      0.41      1535\n",
            "\n",
            "    accuracy                           0.54      3639\n",
            "   macro avg       0.52      0.52      0.52      3639\n",
            "weighted avg       0.53      0.54      0.53      3639\n",
            "\n",
            "confusion matrix:\n",
            "[[1383  721]\n",
            " [ 955  580]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZYb-g0-mkHA",
        "outputId": "3438a954-3dc8-4d7f-c9b9-fa6ede3e7095"
      },
      "source": [
        "clf = GradientBoostingClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "# compute the performance measures\n",
        "score1 = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"accuracy:   %0.3f\" % score1)\n",
        "\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"confusion matrix:\")\n",
        "print(metrics.confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy:   0.562\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.93      0.71      2076\n",
            "           1       0.46      0.08      0.13      1581\n",
            "\n",
            "    accuracy                           0.56      3657\n",
            "   macro avg       0.51      0.50      0.42      3657\n",
            "weighted avg       0.52      0.56      0.46      3657\n",
            "\n",
            "confusion matrix:\n",
            "[[1936  140]\n",
            " [1462  119]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjQOhOt8lDRi",
        "outputId": "2bbf448e-8942-4c9c-e901-7b002cc4ead0"
      },
      "source": [
        "clf = PassiveAggressiveClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "# compute the performance measures\n",
        "score1 = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"accuracy:   %0.3f\" % score1)\n",
        "\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"confusion matrix:\")\n",
        "print(metrics.confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy:   0.456\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.23      0.32      2076\n",
            "           1       0.43      0.75      0.55      1581\n",
            "\n",
            "    accuracy                           0.46      3657\n",
            "   macro avg       0.49      0.49      0.43      3657\n",
            "weighted avg       0.50      0.46      0.42      3657\n",
            "\n",
            "confusion matrix:\n",
            "[[ 478 1598]\n",
            " [ 390 1191]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-dKe_fouAUo"
      },
      "source": [
        "---------------Machine Learning Approaches End--------------------------\n",
        "\n",
        "\n",
        "LSTM on tokenized text\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNEfh2HNpW-8"
      },
      "source": [
        "max_features=4500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj7-Tq--tk1H"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtzZFly4Anwu"
      },
      "source": [
        "X = df['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZDKQrTstU0Z"
      },
      "source": [
        "tokenizer = Tokenizer(num_words = max_features, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ')\n",
        "tokenizer.fit_on_texts(texts = X)\n",
        "X = tokenizer.texts_to_sequences(texts = X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O07eUlbEt0LS"
      },
      "source": [
        "X = pad_sequences(sequences = X, maxlen = max_features, padding = 'pre')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4imIb-_et8Of"
      },
      "source": [
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS8gU1CznsUi"
      },
      "source": [
        "lstm_model = Sequential(name = 'lstm_nn_model')\n",
        "lstm_model.add(layer = Embedding(input_dim = max_features, output_dim = 120, name = '1st_layer'))\n",
        "lstm_model.add(layer = LSTM(units = 120, dropout = 0.2, recurrent_dropout = 0.2, name = '2nd_layer'))\n",
        "lstm_model.add(layer = Dropout(rate = 0.5, name = '3rd_layer'))\n",
        "lstm_model.add(layer = Dense(units = 120,  activation = 'relu', name = '4th_layer'))\n",
        "lstm_model.add(layer = Dropout(rate = 0.5, name = '5th_layer'))\n",
        "lstm_model.add(layer = Dense(units = len(set(y)),  activation = 'sigmoid', name = 'output_layer'))\n",
        "# compiling the model\n",
        "lstm_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHueew_wqUbc",
        "outputId": "d2bbcde5-7517-471c-e26a-9785ece2180e"
      },
      "source": [
        "lstm_model_fit = lstm_model.fit(X_train1, y_train1, epochs = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "383/383 [==============================] - 5663s 15s/step - loss: 0.3290 - accuracy: 0.8531\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcO7Azvwu-bu"
      },
      "source": [
        "y_pred = lstm_model_fit.predict(X_test1)\n",
        "score1 = metrics.accuracy_score(y_test1, y_pred)\n",
        "print(\"accuracy:   %0.3f\" % score1)\n",
        "print(metrics.classification_report(y_test1, y_pred))\n",
        "print(\"confusion matrix:\")\n",
        "print(metrics.confusion_matrix(y_test1, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcrWuT46vZyR"
      },
      "source": [
        "GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvigVqTovZH5"
      },
      "source": [
        "gru_model = Sequential(name = 'gru_nn_model')\n",
        "gru_model.add(layer = Embedding(input_dim = max_features, output_dim = 120, name = '1st_layer'))\n",
        "gru_model.add(layer = GRU(units = 120, dropout = 0.2, \n",
        "                          recurrent_dropout = 0.2, recurrent_activation = 'relu', \n",
        "                          activation = 'relu', name = '2nd_layer'))\n",
        "gru_model.add(layer = Dropout(rate = 0.4, name = '3rd_layer'))\n",
        "gru_model.add(layer = Dense(units = 120, activation = 'relu', name = '4th_layer'))\n",
        "gru_model.add(layer = Dropout(rate = 0.2, name = '5th_layer'))\n",
        "gru_model.add(layer = Dense(units = len(set(y)), activation = 'softmax', name = 'output_layer'))\n",
        "# compiling the model\n",
        "gru_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Uvp6_qDvcRk"
      },
      "source": [
        "gru = gru_model.fit(X_train, y_train, epochs = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkISKvnUvi5l"
      },
      "source": [
        "y_pred = gru.predict(X_test)\n",
        "score1 = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"accuracy:   %0.3f\" % score1)\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "print(\"confusion matrix:\")\n",
        "print(metrics.confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}