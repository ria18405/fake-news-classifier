{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec_NLP_Proj.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5LF7xmlrn3S",
        "outputId": "04abe5ff-e15e-41ff-c469-dd5651f3c3ec"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from textblob import TextBlob, Word, Blobber\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "# print(stopwords.words('english'))\n",
        "import plotly.express as px\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "nltk.download('stopwords')\n",
        "from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, RepeatedStratifiedKFold, RandomizedSearchCV, LeavePOut, StratifiedKFold\n",
        "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, RandomForestClassifier, BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.metrics as metrics"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WR1hr0PKpDlY"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1eGjGKUNKXq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqxJmtHPDd-i"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/NLP/NLP project/train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzFWvL-CEu-R"
      },
      "source": [
        "df = df.dropna()\n",
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8J3sADXV3CRr",
        "outputId": "704245d8-81e0-443d-83c5-c8765aeaf568"
      },
      "source": [
        "from tqdm.notebook import tqdm, trange\n",
        "from nltk.stem import PorterStemmer\n",
        "!pip install autocorrect\n",
        "from autocorrect import Speller\n",
        "from autocorrect import Speller\n",
        "\n",
        "spell = Speller(lang='en')\n",
        "\n",
        "porter = PorterStemmer()\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "def lower_text(text):\n",
        "    return text.lower()\n",
        "\n",
        "def stemSentence(sentence):\n",
        "    token_words=word_tokenize(sentence)\n",
        "    token_words\n",
        "    stem_sentence=[]\n",
        "    for word in token_words:\n",
        "        stem_sentence.append(porter.stem(word))\n",
        "        stem_sentence.append(\" \")\n",
        "    return \"\".join(stem_sentence)\n",
        "\n",
        "def lemmatization(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "\n",
        "def remove_numbers(text):\n",
        "    text_cleaned = re.sub('[^a-zA-Z]',' ',text)\n",
        "    return text_cleaned\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stoplist = stopwords.words('english')\n",
        "    sps = stoplist\n",
        "    return \" \".join([word for word in str(text).split() if word not in sps])\n",
        "\n",
        "def autospell(text):\n",
        "    return \" \".join([spell(word) for word in text.split()])\n",
        "\n",
        "corpus=[]\n",
        "def get_corpus(sentence):\n",
        "    cps = []\n",
        "    token_words=word_tokenize(sentence)\n",
        "    for word in token_words:\n",
        "        corpus.append(word)\n",
        "    return sentence\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: autocorrect in /usr/local/lib/python3.7/dist-packages (2.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwJt25e23EBj"
      },
      "source": [
        "df['text'] = df['text'].apply(remove_stopwords)\n",
        "df['title'] = df['title'].apply(remove_stopwords)\n",
        "df['author'] = df['author'].apply(remove_stopwords)\n",
        "\n",
        "df['text'] = df['text'].apply(remove_numbers)\n",
        "df['title'] = df['title'].apply(remove_numbers)\n",
        "df['author'] = df['author'].apply(remove_numbers)\n",
        "\n",
        "df['text'] = df['text'].apply(lemmatization)\n",
        "df['title'] = df['title'].apply(lemmatization)\n",
        "df['author'] = df['author'].apply(lemmatization)\n",
        "\n",
        "df['text'] = df['text'].apply(lower_text)\n",
        "df['title'] = df['title'].apply(lower_text)\n",
        "df['author'] = df['author'].apply(lower_text)\n",
        "\n",
        "# df['text'] = df['text'].apply(get_corpus)\n",
        "# df['title'] = df['title'].apply(get_corpus)\n",
        "# df['author'] = df['author'].apply(get_corpus)\n",
        "# corpus = set(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tu5t3mDf7LyA",
        "outputId": "974c8777-bb08-4fa6-b66a-0a7c05c9e58b"
      },
      "source": [
        "# dropping length 0, 1, 2 texts\n",
        "ohr = []\n",
        "for index in range(len(df)):\n",
        "    # ohr.append(len(onehot_rep_test[index]))\n",
        "    if len(df['text'][index])==0 or len(df['text'][index])==1 or len(df['text'][index])==2:\n",
        "        ohr.append(index)\n",
        "print(len(ohr), len(df))\n",
        "df = df.drop(ohr)        \n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "print(len(df))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "93 18285\n",
            "18192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGr8YY6oHJx9"
      },
      "source": [
        "df['titleplustext'] = df['text'] + \" \" +df['title']\n",
        "df['authorplustext'] = df['text'] + \" \" +df['author']  \n",
        "df['titleauthortext'] = df['text'] + \" \" +df['title'] + \" \" +df['author']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bkkkmYTG7qR"
      },
      "source": [
        "df.to_csv(\"/content/drive/MyDrive/NLP/NLP project/filtered_df.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "E3lMADnLJ0de",
        "outputId": "3c634f0a-b5ff-4ac3-b6b5-ad2bd7bcf851"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>titleplustext</th>\n",
              "      <th>authorplustext</th>\n",
              "      <th>titleauthortext</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>16753</td>\n",
              "      <td>rick perry ex governor texas is trump s pick e...</td>\n",
              "      <td>coral davenport</td>\n",
              "      <td>washington donald j trump plan name rick perry...</td>\n",
              "      <td>0</td>\n",
              "      <td>washington donald j trump plan name rick perry...</td>\n",
              "      <td>washington donald j trump plan name rick perry...</td>\n",
              "      <td>washington donald j trump plan name rick perry...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6909</td>\n",
              "      <td>deepwater horizon continues impact public health</td>\n",
              "      <td>admin</td>\n",
              "      <td>deepwater horizon continues impact public heal...</td>\n",
              "      <td>1</td>\n",
              "      <td>deepwater horizon continues impact public heal...</td>\n",
              "      <td>deepwater horizon continues impact public heal...</td>\n",
              "      <td>deepwater horizon continues impact public heal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10842</td>\n",
              "      <td>nikki haley puts u n notice u s is taking name...</td>\n",
              "      <td>somini sengupta</td>\n",
              "      <td>united nations the american ambassador united ...</td>\n",
              "      <td>0</td>\n",
              "      <td>united nations the american ambassador united ...</td>\n",
              "      <td>united nations the american ambassador united ...</td>\n",
              "      <td>united nations the american ambassador united ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>19326</td>\n",
              "      <td>what hillary clinton india not much here s why</td>\n",
              "      <td>saurav dutt</td>\n",
              "      <td>bill clinton cabinet member pursued anti india...</td>\n",
              "      <td>1</td>\n",
              "      <td>bill clinton cabinet member pursued anti india...</td>\n",
              "      <td>bill clinton cabinet member pursued anti india...</td>\n",
              "      <td>bill clinton cabinet member pursued anti india...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17547</td>\n",
              "      <td>donald trump announces urban revitalization pl...</td>\n",
              "      <td>iwb</td>\n",
              "      <td>dr darrell scott pastordscott october seriousl...</td>\n",
              "      <td>1</td>\n",
              "      <td>dr darrell scott pastordscott october seriousl...</td>\n",
              "      <td>dr darrell scott pastordscott october seriousl...</td>\n",
              "      <td>dr darrell scott pastordscott october seriousl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18187</th>\n",
              "      <td>16492</td>\n",
              "      <td>gary johnson s what is aleppo flub amplifies s...</td>\n",
              "      <td>alan rappeport</td>\n",
              "      <td>when accepted libertarian party s nomination m...</td>\n",
              "      <td>0</td>\n",
              "      <td>when accepted libertarian party s nomination m...</td>\n",
              "      <td>when accepted libertarian party s nomination m...</td>\n",
              "      <td>when accepted libertarian party s nomination m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18188</th>\n",
              "      <td>1327</td>\n",
              "      <td>mike allen politico s newsletter pioneer is ha...</td>\n",
              "      <td>jim rutenberg</td>\n",
              "      <td>siren washington to lose its daily bard at a m...</td>\n",
              "      <td>0</td>\n",
              "      <td>siren washington to lose its daily bard at a m...</td>\n",
              "      <td>siren washington to lose its daily bard at a m...</td>\n",
              "      <td>siren washington to lose its daily bard at a m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18189</th>\n",
              "      <td>1700</td>\n",
              "      <td>student sues police over baton encounter brook...</td>\n",
              "      <td>rick rojas</td>\n",
              "      <td>an brooklyn high school student sued new york ...</td>\n",
              "      <td>0</td>\n",
              "      <td>an brooklyn high school student sued new york ...</td>\n",
              "      <td>an brooklyn high school student sued new york ...</td>\n",
              "      <td>an brooklyn high school student sued new york ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18190</th>\n",
              "      <td>7557</td>\n",
              "      <td>hillary clinton united nations same open borde...</td>\n",
              "      <td>kaitlyn stegall</td>\n",
              "      <td>october hillary clinton united nations same op...</td>\n",
              "      <td>1</td>\n",
              "      <td>october hillary clinton united nations same op...</td>\n",
              "      <td>october hillary clinton united nations same op...</td>\n",
              "      <td>october hillary clinton united nations same op...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18191</th>\n",
              "      <td>20360</td>\n",
              "      <td>the secret good toast it s your freezer the ne...</td>\n",
              "      <td>emily weinstein</td>\n",
              "      <td>toast lover i modest proposal you do bother ba...</td>\n",
              "      <td>0</td>\n",
              "      <td>toast lover i modest proposal you do bother ba...</td>\n",
              "      <td>toast lover i modest proposal you do bother ba...</td>\n",
              "      <td>toast lover i modest proposal you do bother ba...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>18192 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  ...                                    titleauthortext\n",
              "0      16753  ...  washington donald j trump plan name rick perry...\n",
              "1       6909  ...  deepwater horizon continues impact public heal...\n",
              "2      10842  ...  united nations the american ambassador united ...\n",
              "3      19326  ...  bill clinton cabinet member pursued anti india...\n",
              "4      17547  ...  dr darrell scott pastordscott october seriousl...\n",
              "...      ...  ...                                                ...\n",
              "18187  16492  ...  when accepted libertarian party s nomination m...\n",
              "18188   1327  ...  siren washington to lose its daily bard at a m...\n",
              "18189   1700  ...  an brooklyn high school student sued new york ...\n",
              "18190   7557  ...  october hillary clinton united nations same op...\n",
              "18191  20360  ...  toast lover i modest proposal you do bother ba...\n",
              "\n",
              "[18192 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaZzVsyaNTXK"
      },
      "source": [
        "df  = pd.read_csv(\"/content/drive/MyDrive/NLP/NLP project/filtered_df.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxG7R9HOzlc6"
      },
      "source": [
        "X = df['text']\n",
        "y = df['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
        "X_train = X_train.sample(frac=1).reset_index(drop=True)\n",
        "X_test = X_test.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOwbqmBPNgoh"
      },
      "source": [
        "def return_corpus(all_sentences):\n",
        "    corpus=[]\n",
        "    for sentence in all_sentences:\n",
        "        token_words=word_tokenize(sentence)\n",
        "        corpus.extend(token_words)\n",
        "    return corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TD55GSt6zyj",
        "outputId": "4c859e8e-e8f3-414e-db1e-f7926d349191"
      },
      "source": [
        "train_corpus = return_corpus(X_train)\n",
        "test_corpus = return_corpus(X_test)\n",
        "train_corpus = set(train_corpus)\n",
        "test_corpus = set(test_corpus)\n",
        "len(train_corpus), len(test_corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(121892, 60713)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaAwTL8bMQYZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24da9c64-7a59-4958-9970-fcb733c061db"
      },
      "source": [
        "import gensim\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "def get_embs_vocab(model, corpus):\n",
        "    embeddings = {}\n",
        "    for word in corpus:\n",
        "        word = word.lower()\n",
        "        try:\n",
        "            embeddings[word] = model[word]\n",
        "        except:\n",
        "            words = list(gensim.utils.tokenize(word))\n",
        "            try:\n",
        "                embs = [model[i] for i in words]\n",
        "                emb = sum(embs)\n",
        "                if emb ==0:\n",
        "                    embeddings[word] = np.zeros(sh)\n",
        "                else:\n",
        "                    embeddings[word] = emb\n",
        "            except:\n",
        "                embeddings[word] = np.zeros(vec_king.shape)\n",
        "    return embeddings\n",
        "wv = api.load('word2vec-google-news-300')\n",
        "# get_embs_vocab(wv, corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGNN_Qoa8BlB"
      },
      "source": [
        "vec_king = wv['king']\n",
        "sh = vec_king.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6ZrADUPFwMb",
        "outputId": "f025219b-11b8-48d9-a97e-8672aec9c569"
      },
      "source": [
        "train_word2vec_embeddings = get_embs_vocab(wv, train_corpus)\n",
        "test_word2vec_embeddings = get_embs_vocab(wv, test_corpus)\n",
        "len(train_word2vec_embeddings), len(test_word2vec_embeddings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(121892, 60713)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNTiox_YxYS2"
      },
      "source": [
        "paddinglen = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A254G7yWxsqY"
      },
      "source": [
        "train_embed = []\n",
        "for sentence in X_train:\n",
        "    token_words=word_tokenize(sentence)\n",
        "    if len(token_words)<paddinglen:\n",
        "        tobeadded = [np.zeros(sh)]*(paddinglen-len(token_words))\n",
        "        # print(len(tobeadded), tobeadded[0].shape)\n",
        "        sentence_embed = [train_word2vec_embeddings[word] for word in token_words]\n",
        "        sentence_embed.extend(tobeadded)\n",
        "    else:\n",
        "        token_words=token_words[:paddinglen]\n",
        "        sentence_embed = [train_word2vec_embeddings[word] for word in token_words]\n",
        "    train_embed.append(sentence_embed)\n",
        "train_embed = np.asarray(train_embed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZRQttMGU85G"
      },
      "source": [
        "test_embed = []\n",
        "for sentence in X_test:\n",
        "    token_words=word_tokenize(sentence)\n",
        "    if len(token_words)<paddinglen:\n",
        "        tobeadded = [np.zeros(sh)]*(paddinglen-len(token_words))\n",
        "        # print(len(tobeadded), tobeadded[0].shape)\n",
        "        sentence_embed = [test_word2vec_embeddings[word] for word in token_words]\n",
        "        sentence_embed.extend(tobeadded)\n",
        "    else:\n",
        "        token_words=token_words[:paddinglen]\n",
        "        sentence_embed = [test_word2vec_embeddings[word] for word in token_words]\n",
        "    test_embed.append(sentence_embed)\n",
        "test_embed = np.asarray(test_embed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne5Z6RND921J"
      },
      "source": [
        "train_embed.shape, test_embed.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Nr1_fQS2Vuf"
      },
      "source": [
        "X_train = train_embed.reshape(train_embed.shape[0], train_embed.shape[1]*train_embed.shape[2])\n",
        "X_test = test_embed.reshape(test_embed.shape[0], test_embed.shape[1]*test_embed.shape[2])\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8BHZrAhAM0l"
      },
      "source": [
        "np.save(X_train, \"/content/drive/MyDrive/NLP/NLP project/train_x.npy\")\n",
        "np.save(X_test, \"/content/drive/MyDrive/NLP/NLP project/test_x.npy\")\n",
        "np.save(y_train, \"/content/drive/MyDrive/NLP/NLP project/train_y.npy\")\n",
        "np.save(y_test, \"/content/drive/MyDrive/NLP/NLP project/test_y.npy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujVM92NqrX1a"
      },
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "clf = make_pipeline(SVC(gamma='auto'))\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# compute the performance measures\n",
        "score1 = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"accuracy:   %0.3f\" % score1)\n",
        "\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"confusion matrix:\")\n",
        "print(metrics.confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print('------------------------------')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3moKcFpkGZS"
      },
      "source": [
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "# compute the performance measures\n",
        "score1 = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"accuracy:   %0.3f\" % score1)\n",
        "\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"confusion matrix:\")\n",
        "print(metrics.confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiJ21hNYlHIk",
        "outputId": "a09dbfd6-f9cf-42e0-ed8d-1a39a04f9ea8"
      },
      "source": [
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# compute the performance measures\n",
        "score1 = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"accuracy:   %0.3f\" % score1)\n",
        "\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"confusion matrix:\")\n",
        "print(metrics.confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy:   0.552\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.89      0.69      2076\n",
            "           1       0.43      0.11      0.18      1581\n",
            "\n",
            "    accuracy                           0.55      3657\n",
            "   macro avg       0.50      0.50      0.43      3657\n",
            "weighted avg       0.51      0.55      0.47      3657\n",
            "\n",
            "confusion matrix:\n",
            "[[1842  234]\n",
            " [1404  177]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4WUkRTJlaJK",
        "outputId": "c65fac26-a824-485c-dc1c-0e8c5aed2e74"
      },
      "source": [
        "clf = AdaBoostClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "# compute the performance measures\n",
        "score1 = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"accuracy:   %0.3f\" % score1)\n",
        "\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"confusion matrix:\")\n",
        "print(metrics.confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy:   0.546\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.82      0.67      2076\n",
            "           1       0.44      0.18      0.26      1581\n",
            "\n",
            "    accuracy                           0.55      3657\n",
            "   macro avg       0.51      0.50      0.47      3657\n",
            "weighted avg       0.51      0.55      0.49      3657\n",
            "\n",
            "confusion matrix:\n",
            "[[1707  369]\n",
            " [1290  291]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UczphPWnmD3c",
        "outputId": "bd609f55-1ad1-4410-ca0d-022bab3e5f82"
      },
      "source": [
        "clf = MLPClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "# compute the performance measures\n",
        "score1 = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"accuracy:   %0.3f\" % score1)\n",
        "\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"confusion matrix:\")\n",
        "print(metrics.confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy:   0.467\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.28      0.37      2076\n",
            "           1       0.43      0.72      0.54      1581\n",
            "\n",
            "    accuracy                           0.47      3657\n",
            "   macro avg       0.50      0.50      0.45      3657\n",
            "weighted avg       0.51      0.47      0.44      3657\n",
            "\n",
            "confusion matrix:\n",
            "[[ 573 1503]\n",
            " [ 446 1135]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZYb-g0-mkHA",
        "outputId": "3438a954-3dc8-4d7f-c9b9-fa6ede3e7095"
      },
      "source": [
        "clf = GradientBoostingClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "# compute the performance measures\n",
        "score1 = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"accuracy:   %0.3f\" % score1)\n",
        "\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"confusion matrix:\")\n",
        "print(metrics.confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy:   0.562\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.93      0.71      2076\n",
            "           1       0.46      0.08      0.13      1581\n",
            "\n",
            "    accuracy                           0.56      3657\n",
            "   macro avg       0.51      0.50      0.42      3657\n",
            "weighted avg       0.52      0.56      0.46      3657\n",
            "\n",
            "confusion matrix:\n",
            "[[1936  140]\n",
            " [1462  119]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjQOhOt8lDRi",
        "outputId": "2bbf448e-8942-4c9c-e901-7b002cc4ead0"
      },
      "source": [
        "clf = PassiveAggressiveClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "# compute the performance measures\n",
        "score1 = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"accuracy:   %0.3f\" % score1)\n",
        "\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"confusion matrix:\")\n",
        "print(metrics.confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy:   0.456\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.23      0.32      2076\n",
            "           1       0.43      0.75      0.55      1581\n",
            "\n",
            "    accuracy                           0.46      3657\n",
            "   macro avg       0.49      0.49      0.43      3657\n",
            "weighted avg       0.50      0.46      0.42      3657\n",
            "\n",
            "confusion matrix:\n",
            "[[ 478 1598]\n",
            " [ 390 1191]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-dKe_fouAUo"
      },
      "source": [
        "LSTM on tokenized text\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNEfh2HNpW-8"
      },
      "source": [
        "max_features=4500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj7-Tq--tk1H"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtzZFly4Anwu"
      },
      "source": [
        "X = df['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZDKQrTstU0Z"
      },
      "source": [
        "tokenizer = Tokenizer(num_words = max_features, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ')\n",
        "tokenizer.fit_on_texts(texts = X)\n",
        "X = tokenizer.texts_to_sequences(texts = X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O07eUlbEt0LS"
      },
      "source": [
        "X = pad_sequences(sequences = X, maxlen = max_features, padding = 'pre')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4imIb-_et8Of"
      },
      "source": [
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS8gU1CznsUi"
      },
      "source": [
        "lstm_model = Sequential(name = 'lstm_nn_model')\n",
        "lstm_model.add(layer = Embedding(input_dim = max_features, output_dim = 120, name = '1st_layer'))\n",
        "lstm_model.add(layer = LSTM(units = 120, dropout = 0.2, recurrent_dropout = 0.2, name = '2nd_layer'))\n",
        "lstm_model.add(layer = Dropout(rate = 0.5, name = '3rd_layer'))\n",
        "lstm_model.add(layer = Dense(units = 120,  activation = 'relu', name = '4th_layer'))\n",
        "lstm_model.add(layer = Dropout(rate = 0.5, name = '5th_layer'))\n",
        "lstm_model.add(layer = Dense(units = len(set(y)),  activation = 'sigmoid', name = 'output_layer'))\n",
        "# compiling the model\n",
        "lstm_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHueew_wqUbc",
        "outputId": "0b2936f2-af53-4127-c931-a06686098fb3"
      },
      "source": [
        "lstm_model_fit = lstm_model.fit(X_train1, y_train1, epochs = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 83/383 [=====>........................] - ETA: 1:12:41 - loss: 0.5508 - accuracy: 0.6962"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcO7Azvwu-bu"
      },
      "source": [
        "y_pred = lstm_model_fit.predict(X_test1)\n",
        "score1 = metrics.accuracy_score(y_test1, y_pred)\n",
        "print(\"accuracy:   %0.3f\" % score1)\n",
        "print(metrics.classification_report(y_test1, y_pred))\n",
        "print(\"confusion matrix:\")\n",
        "print(metrics.confusion_matrix(y_test1, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcrWuT46vZyR"
      },
      "source": [
        "GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvigVqTovZH5"
      },
      "source": [
        "gru_model = Sequential(name = 'gru_nn_model')\n",
        "gru_model.add(layer = Embedding(input_dim = max_features, output_dim = 120, name = '1st_layer'))\n",
        "gru_model.add(layer = GRU(units = 120, dropout = 0.2, \n",
        "                          recurrent_dropout = 0.2, recurrent_activation = 'relu', \n",
        "                          activation = 'relu', name = '2nd_layer'))\n",
        "gru_model.add(layer = Dropout(rate = 0.4, name = '3rd_layer'))\n",
        "gru_model.add(layer = Dense(units = 120, activation = 'relu', name = '4th_layer'))\n",
        "gru_model.add(layer = Dropout(rate = 0.2, name = '5th_layer'))\n",
        "gru_model.add(layer = Dense(units = len(set(y)), activation = 'softmax', name = 'output_layer'))\n",
        "# compiling the model\n",
        "gru_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Uvp6_qDvcRk"
      },
      "source": [
        "gru = gru_model.fit(X_train, y_train, epochs = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkISKvnUvi5l"
      },
      "source": [
        "y_pred = gru.predict(X_test)\n",
        "score1 = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"accuracy:   %0.3f\" % score1)\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "print(\"confusion matrix:\")\n",
        "print(metrics.confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}