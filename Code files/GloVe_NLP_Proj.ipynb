{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GloVe_NLP_Proj.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5LF7xmlrn3S",
        "outputId": "6b9ca43c-1ee1-4834-fca7-a01798482e33"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from textblob import TextBlob, Word, Blobber\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "# print(stopwords.words('english'))\n",
        "import plotly.express as px\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "nltk.download('stopwords')\n",
        "from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, RepeatedStratifiedKFold, RandomizedSearchCV, LeavePOut, StratifiedKFold\n",
        "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, RandomForestClassifier, BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.metrics as metrics"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WR1hr0PKpDlY",
        "outputId": "95f29757-1a16-42e5-838b-120577b5583b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqxJmtHPDd-i"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/NLP/NLP project/train.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzFWvL-CEu-R"
      },
      "source": [
        "df = df.dropna()\n",
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8J3sADXV3CRr",
        "outputId": "c185cce4-8975-4085-fe0c-086ab1228af4"
      },
      "source": [
        "from tqdm.notebook import tqdm, trange\n",
        "from nltk.stem import PorterStemmer\n",
        "!pip install autocorrect\n",
        "from autocorrect import Speller\n",
        "from autocorrect import Speller\n",
        "\n",
        "spell = Speller(lang='en')\n",
        "\n",
        "porter = PorterStemmer()\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "def lower_text(text):\n",
        "    return text.lower()\n",
        "\n",
        "def stemSentence(sentence):\n",
        "    token_words=word_tokenize(sentence)\n",
        "    token_words\n",
        "    stem_sentence=[]\n",
        "    for word in token_words:\n",
        "        stem_sentence.append(porter.stem(word))\n",
        "        stem_sentence.append(\" \")\n",
        "    return \"\".join(stem_sentence)\n",
        "\n",
        "def lemmatization(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "\n",
        "def remove_numbers(text):\n",
        "    text_cleaned = re.sub('[^a-zA-Z]',' ',text)\n",
        "    return text_cleaned\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stoplist = stopwords.words('english')\n",
        "    sps = stoplist\n",
        "    return \" \".join([word for word in str(text).split() if word not in sps])\n",
        "\n",
        "def autospell(text):\n",
        "    return \" \".join([spell(word) for word in text.split()])\n",
        "\n",
        "corpus=[]\n",
        "def get_corpus(sentence):\n",
        "    cps = []\n",
        "    token_words=word_tokenize(sentence)\n",
        "    for word in token_words:\n",
        "        corpus.append(word)\n",
        "    return sentence\n",
        "\n",
        "def return_corpus(all_sentences):\n",
        "    corpus=[]\n",
        "    for sentence in all_sentences:\n",
        "        token_words=word_tokenize(sentence)\n",
        "        corpus.extend(token_words)\n",
        "    return corpus"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autocorrect\n",
            "  Downloading autocorrect-2.6.0.tar.gz (622 kB)\n",
            "\u001b[?25l\r\u001b[K     |▌                               | 10 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |█                               | 20 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 30 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██                              | 40 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 51 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 61 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 71 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 81 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 102 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 112 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 122 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 133 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 143 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 153 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 163 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 174 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 184 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 194 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 204 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 215 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 225 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 235 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 245 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 256 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 266 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 276 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 286 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 296 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 307 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 317 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 327 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 337 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 348 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 358 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 368 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 378 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 389 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 399 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 409 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 419 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 430 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 440 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 450 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 460 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 471 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 481 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 491 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 501 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 512 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 522 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 532 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 542 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 552 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 563 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 573 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 583 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 593 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 604 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 614 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 622 kB 4.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.6.0-py3-none-any.whl size=622249 sha256=f95493b0f01bda1b7cf486fef1f2ea7abfdc9c65768fd53d7cecbd17ce7c2486\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/ce/aa/bc894efbe0541ce91dea21561d01d319783986d9787a8e9f58\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwJt25e23EBj"
      },
      "source": [
        "df['text'] = df['text'].apply(remove_stopwords)\n",
        "# df['title'] = df['title'].apply(remove_stopwords)\n",
        "# df['author'] = df['author'].apply(remove_stopwords)\n",
        "\n",
        "df['text'] = df['text'].apply(remove_numbers)\n",
        "# df['title'] = df['title'].apply(remove_numbers)\n",
        "# df['author'] = df['author'].apply(remove_numbers)\n",
        "\n",
        "df['text'] = df['text'].apply(lemmatization)\n",
        "# df['title'] = df['title'].apply(lemmatization)\n",
        "# df['author'] = df['author'].apply(lemmatization)\n",
        "\n",
        "df['text'] = df['text'].apply(lower_text)\n",
        "# df['title'] = df['title'].apply(lower_text)\n",
        "# df['author'] = df['author'].apply(lower_text)\n",
        "\n",
        "# df['text'] = df['text'].apply(get_corpus)\n",
        "# df['title'] = df['title'].apply(get_corpus)\n",
        "# df['author'] = df['author'].apply(get_corpus)\n",
        "# corpus = set(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxG7R9HOzlc6"
      },
      "source": [
        "X = df['text']\n",
        "y = df['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
        "X_train = X_train.sample(frac=1).reset_index(drop=True)\n",
        "X_test = X_test.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaAwTL8bMQYZ"
      },
      "source": [
        "import gensim.downloader as api\n",
        "def create_glove_embedding():\n",
        "    embeddings_dict = {}\n",
        "    with open(\"/content/drive/MyDrive/NLP/NLP project/PretrainedModels/glove.6B.300d.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], \"float32\")\n",
        "            embeddings_dict[word] = vector\n",
        "    return embeddings_dict\n",
        "def get_embs_vocab(model, corpus):\n",
        "    embeddings = {}\n",
        "    for word in corpus:\n",
        "        word = word.lower()\n",
        "        try:\n",
        "            embeddings[word] = model[word]\n",
        "        except:\n",
        "            words = list(gensim.utils.tokenize(word))\n",
        "            try:\n",
        "                embs = [model[i] for i in words]\n",
        "                emb = sum(embs)\n",
        "                if emb ==0:\n",
        "                    embeddings[word] = np.zeros(sh)\n",
        "                else:\n",
        "                    embeddings[word] = emb\n",
        "            except:\n",
        "                embeddings[word] = np.zeros(vec_king.shape)\n",
        "    return embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXiEg93VQb1C",
        "outputId": "04d359ce-b157-4217-fa23-b736f7a96fba"
      },
      "source": [
        "train_corpus = return_corpus(X_train)\n",
        "test_corpus = return_corpus(X_test)\n",
        "train_corpus = set(train_corpus)\n",
        "test_corpus = set(test_corpus)\n",
        "len(train_corpus), len(test_corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(121667, 61138)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT7c5Zc3SE3Q"
      },
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZZH6Ga6Nv7U",
        "outputId": "db3fc951-fea7-4515-bb30-dddf1ff09c00"
      },
      "source": [
        "#glove embeddings\n",
        "glove_embedding_model = create_glove_embedding()\n",
        "vec_king = glove_embedding_model[\"king\"]\n",
        "sh = list(glove_embedding_model.values())[0].shape\n",
        "\n",
        "train_glove_embeddings = get_embs_vocab(glove_embedding_model, train_corpus)\n",
        "test_glove_embeddings = get_embs_vocab(glove_embedding_model, test_corpus)\n",
        "\n",
        "len(test_glove_embeddings), len(train_glove_embeddings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(61138, 121667)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_h9fV70kS9PS"
      },
      "source": [
        "maxlen = 0\n",
        "for sentence in X_train:\n",
        "    token_words=word_tokenize(sentence)\n",
        "    if len(token_words)>maxlen:\n",
        "        maxlen = len(token_words)\n",
        "maxlen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNTiox_YxYS2"
      },
      "source": [
        "paddinglen = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A254G7yWxsqY"
      },
      "source": [
        "train_embed = []\n",
        "for sentence in X_train:\n",
        "    token_words=word_tokenize(sentence)\n",
        "    if len(token_words)<paddinglen:\n",
        "        tobeadded = [np.zeros(sh)]*(paddinglen-len(token_words))\n",
        "        # print(len(tobeadded), tobeadded[0].shape)\n",
        "        sentence_embed = [train_glove_embeddings[word] for word in token_words]\n",
        "        sentence_embed.extend(tobeadded)\n",
        "    else:\n",
        "        token_words=token_words[:paddinglen]\n",
        "        sentence_embed = [train_glove_embeddings[word] for word in token_words]\n",
        "    train_embed.append(sentence_embed)\n",
        "train_embed = np.asarray(train_embed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZRQttMGU85G"
      },
      "source": [
        "test_embed = []\n",
        "for sentence in X_test:\n",
        "    token_words=word_tokenize(sentence)\n",
        "    if len(token_words)<paddinglen:\n",
        "        tobeadded = [np.zeros(sh)]*(paddinglen-len(token_words))\n",
        "        # print(len(tobeadded), tobeadded[0].shape)\n",
        "        sentence_embed = [test_glove_embeddings[word] for word in token_words]\n",
        "        sentence_embed.extend(tobeadded)\n",
        "    else:\n",
        "        token_words=token_words[:paddinglen]\n",
        "        sentence_embed = [test_glove_embeddings[word] for word in token_words]\n",
        "    test_embed.append(sentence_embed)\n",
        "test_embed = np.asarray(test_embed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ne5Z6RND921J",
        "outputId": "5b132b1f-27de-4230-fb70-6e1829e4abf6"
      },
      "source": [
        "train_embed.shape, test_embed.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((14628, 200, 300), (3657, 200, 300))"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Nr1_fQS2Vuf",
        "outputId": "15afd76b-c594-4fc2-fd76-59677b939161"
      },
      "source": [
        "X_train = train_embed.reshape(train_embed.shape[0], train_embed.shape[1]*train_embed.shape[2])\n",
        "X_test = test_embed.reshape(test_embed.shape[0], test_embed.shape[1]*test_embed.shape[2])\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((14628, 60000), (3657, 60000), (14628,), (3657,))"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8BHZrAhAM0l"
      },
      "source": [
        "np.save( \"/content/drive/MyDrive/NLP/NLP project/train_x.npy\", X_train)\n",
        "np.save( \"/content/drive/MyDrive/NLP/NLP project/test_x.npy\", X_test)\n",
        "np.save( \"/content/drive/MyDrive/NLP/NLP project/train_y.npy\", y_train)\n",
        "np.save( \"/content/drive/MyDrive/NLP/NLP project/test_y.npy\", y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Rp23O5refF1H"
      },
      "source": [
        "X_train = np.load( \"/content/drive/MyDrive/NLP/NLP project/train_x.npy\")\n",
        "X_test = np.load( \"/content/drive/MyDrive/NLP/NLP project/test_x.npy\")\n",
        "y_train = np.load( \"/content/drive/MyDrive/NLP/NLP project/train_y.npy\")\n",
        "y_test = np.load( \"/content/drive/MyDrive/NLP/NLP project/test_y.npy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "y5bB4lWibt66",
        "outputId": "944ede51-f6aa-4aea-efc9-428e23597a27"
      },
      "source": [
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((14628, 60000), (3657, 60000), (14628,), (3657,))"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujVM92NqrX1a"
      },
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "clf = make_pipeline(SVC(gamma='auto'))\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# compute the performance measures\n",
        "score1 = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"accuracy:   %0.3f\" % score1)\n",
        "\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"confusion matrix:\")\n",
        "print(metrics.confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print('------------------------------')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3moKcFpkGZS"
      },
      "source": [
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "# compute the performance measures\n",
        "score1 = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"accuracy:   %0.3f\" % score1)\n",
        "\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"confusion matrix:\")\n",
        "print(metrics.confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiJ21hNYlHIk",
        "outputId": "c925c7df-d897-4625-910f-affa08ce3904"
      },
      "source": [
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# compute the performance measures\n",
        "score1 = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"accuracy:   %0.3f\" % score1)\n",
        "\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"confusion matrix:\")\n",
        "print(metrics.confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning:\n",
            "\n",
            "lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy:   0.505\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.61      0.58      2062\n",
            "           1       0.42      0.37      0.40      1595\n",
            "\n",
            "    accuracy                           0.51      3657\n",
            "   macro avg       0.49      0.49      0.49      3657\n",
            "weighted avg       0.50      0.51      0.50      3657\n",
            "\n",
            "confusion matrix:\n",
            "[[1255  807]\n",
            " [1002  593]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4WUkRTJlaJK",
        "outputId": "c65fac26-a824-485c-dc1c-0e8c5aed2e74"
      },
      "source": [
        "clf = AdaBoostClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "# compute the performance measures\n",
        "score1 = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"accuracy:   %0.3f\" % score1)\n",
        "\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"confusion matrix:\")\n",
        "print(metrics.confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy:   0.546\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.82      0.67      2076\n",
            "           1       0.44      0.18      0.26      1581\n",
            "\n",
            "    accuracy                           0.55      3657\n",
            "   macro avg       0.51      0.50      0.47      3657\n",
            "weighted avg       0.51      0.55      0.49      3657\n",
            "\n",
            "confusion matrix:\n",
            "[[1707  369]\n",
            " [1290  291]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UczphPWnmD3c",
        "outputId": "c1bbed78-8bdf-4e42-b4d9-87bf608a0581"
      },
      "source": [
        "clf = MLPClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "# compute the performance measures\n",
        "score1 = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"accuracy:   %0.3f\" % score1)\n",
        "\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"confusion matrix:\")\n",
        "print(metrics.confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy:   0.517\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.68      0.61      2062\n",
            "           1       0.42      0.30      0.35      1595\n",
            "\n",
            "    accuracy                           0.52      3657\n",
            "   macro avg       0.49      0.49      0.48      3657\n",
            "weighted avg       0.50      0.52      0.50      3657\n",
            "\n",
            "confusion matrix:\n",
            "[[1408  654]\n",
            " [1114  481]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZYb-g0-mkHA"
      },
      "source": [
        "clf = GradientBoostingClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "# compute the performance measures\n",
        "score1 = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"accuracy:   %0.3f\" % score1)\n",
        "\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"confusion matrix:\")\n",
        "print(metrics.confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjQOhOt8lDRi",
        "outputId": "40250292-b77f-4220-c712-1960b7556fb8"
      },
      "source": [
        "clf = PassiveAggressiveClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "# compute the performance measures\n",
        "score1 = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"accuracy:   %0.3f\" % score1)\n",
        "\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"confusion matrix:\")\n",
        "print(metrics.confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy:   0.497\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.52      0.54      2062\n",
            "           1       0.43      0.47      0.45      1595\n",
            "\n",
            "    accuracy                           0.50      3657\n",
            "   macro avg       0.49      0.49      0.49      3657\n",
            "weighted avg       0.50      0.50      0.50      3657\n",
            "\n",
            "confusion matrix:\n",
            "[[1072  990]\n",
            " [ 851  744]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rac9tuLuYa4l"
      },
      "source": [
        "LSTM on tokenised text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvXfSyk2Yd9B"
      },
      "source": [
        "max_features=4500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj7-Tq--tk1H"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtzZFly4Anwu"
      },
      "source": [
        "X = df['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZDKQrTstU0Z"
      },
      "source": [
        "tokenizer = Tokenizer(num_words = max_features, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ')\n",
        "tokenizer.fit_on_texts(texts = X)\n",
        "X = tokenizer.texts_to_sequences(texts = X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O07eUlbEt0LS"
      },
      "source": [
        "X = pad_sequences(sequences = X, maxlen = max_features, padding = 'pre')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4imIb-_et8Of"
      },
      "source": [
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS8gU1CznsUi"
      },
      "source": [
        "lstm_model = Sequential(name = 'lstm_nn_model')\n",
        "lstm_model.add(layer = Embedding(input_dim = max_features, output_dim = 120, name = '1st_layer'))\n",
        "lstm_model.add(layer = LSTM(units = 120, dropout = 0.2, recurrent_dropout = 0.2, name = '2nd_layer'))\n",
        "lstm_model.add(layer = Dropout(rate = 0.5, name = '3rd_layer'))\n",
        "lstm_model.add(layer = Dense(units = 120,  activation = 'relu', name = '4th_layer'))\n",
        "lstm_model.add(layer = Dropout(rate = 0.5, name = '5th_layer'))\n",
        "lstm_model.add(layer = Dense(units = len(set(y)),  activation = 'sigmoid', name = 'output_layer'))\n",
        "# compiling the model\n",
        "lstm_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHueew_wqUbc",
        "outputId": "0b2936f2-af53-4127-c931-a06686098fb3"
      },
      "source": [
        "lstm_model_fit = lstm_model.fit(X_train1, y_train1, epochs = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 83/383 [=====>........................] - ETA: 1:12:41 - loss: 0.5508 - accuracy: 0.6962"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcO7Azvwu-bu"
      },
      "source": [
        "y_pred = lstm_model_fit.predict(X_test1)\n",
        "score1 = metrics.accuracy_score(y_test1, y_pred)\n",
        "print(\"accuracy:   %0.3f\" % score1)\n",
        "print(metrics.classification_report(y_test1, y_pred))\n",
        "print(\"confusion matrix:\")\n",
        "print(metrics.confusion_matrix(y_test1, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcrWuT46vZyR"
      },
      "source": [
        "GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvigVqTovZH5"
      },
      "source": [
        "gru_model = Sequential(name = 'gru_nn_model')\n",
        "gru_model.add(layer = Embedding(input_dim = max_features, output_dim = 120, name = '1st_layer'))\n",
        "gru_model.add(layer = GRU(units = 120, dropout = 0.2, \n",
        "                          recurrent_dropout = 0.2, recurrent_activation = 'relu', \n",
        "                          activation = 'relu', name = '2nd_layer'))\n",
        "gru_model.add(layer = Dropout(rate = 0.4, name = '3rd_layer'))\n",
        "gru_model.add(layer = Dense(units = 120, activation = 'relu', name = '4th_layer'))\n",
        "gru_model.add(layer = Dropout(rate = 0.2, name = '5th_layer'))\n",
        "gru_model.add(layer = Dense(units = len(set(y)), activation = 'softmax', name = 'output_layer'))\n",
        "# compiling the model\n",
        "gru_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Uvp6_qDvcRk"
      },
      "source": [
        "gru = gru_model.fit(X_train, y_train, epochs = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkISKvnUvi5l"
      },
      "source": [
        "y_pred = gru.predict(X_test)\n",
        "score1 = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"accuracy:   %0.3f\" % score1)\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "print(\"confusion matrix:\")\n",
        "print(metrics.confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}